{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32J9_A3d9vgi"
      },
      "source": [
        "# Efficient Object Detection with YOLOV8 and KerasCV\n",
        "\n",
        "**Author:** [Gitesh Chawda](https://twitter.com/gitesh12_)<br>\n",
        "**Date created:** 2023/06/26<br>\n",
        "**Last modified:** 2023/06/26<br>\n",
        "**Description:** Train custom YOLOV8 object detection model with KerasCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0gnHhhm9vgk"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIOFwCnL9vgl"
      },
      "source": [
        "KerasCV is an extension of Keras for computer vision tasks. In this example, we'll see\n",
        "how to train a YOLOV8 object detection model using KerasCV.\n",
        "\n",
        "KerasCV includes pre-trained models for popular computer vision datasets, such as\n",
        "ImageNet, COCO, and Pascal VOC, which can be used for transfer learning. KerasCV also\n",
        "provides a range of visualization tools for inspecting the intermediate representations\n",
        "learned by the model and for visualizing the results of object detection and segmentation\n",
        "tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGFhvFU19vgl"
      },
      "source": [
        "If you're interested in learning about object detection using KerasCV, I highly suggest\n",
        "taking a look at the guide created by lukewood. This resource, available at\n",
        "[Object Detection With KerasCV](https://keras.io/guides/keras_cv/object_detection_keras_cv/#object-detection-introduction),\n",
        "provides a comprehensive overview of the fundamental concepts and techniques\n",
        "required for building object detection models with KerasCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUJvcS3L9vgm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/keras-team/keras-cv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYXAQXRv9vgn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMTcKKGH9vgn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "from keras_cv import visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cisElON49vgn"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MB7rXk69vgn"
      },
      "source": [
        "For this guide, we will be utilizing the Self-Driving Car Dataset obtained from\n",
        "[roboflow](https://public.roboflow.com/object-detection/self-driving-car). In order to\n",
        "make the dataset more manageable, I have extracted a subset of the larger dataset, which\n",
        "originally consisted of 15,000 data samples. From this subset, I have chosen 7,316\n",
        "samples for model training.\n",
        "\n",
        "To simplify the task at hand and focus our efforts, we will be working with a reduced\n",
        "number of object classes. Specifically, we will be considering five primary classes for\n",
        "detection and classification: car, pedestrian, traffic light, biker, and truck. These\n",
        "classes represent some of the most common and significant objects encountered in the\n",
        "context of self-driving cars.\n",
        "\n",
        "By narrowing down the dataset to these specific classes, we can concentrate on building a\n",
        "robust object detection model that can accurately identify and classify these important\n",
        "objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTdlzXN9vgo"
      },
      "source": [
        "The TensorFlow Datasets library provides a convenient way to download and use various\n",
        "datasets, including the object detection dataset. This can be a great option for those\n",
        "who want to quickly start working with the data without having to manually download and\n",
        "preprocess it.\n",
        "\n",
        "You can view various object detection datasets here\n",
        "[TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview#object_detection)\n",
        "\n",
        "However, in this code example, we will demonstrate how to load the dataset from scratch\n",
        "using TensorFlow's `tf.data` pipeline. This approach provides more flexibility and allows\n",
        "you to customize the preprocessing steps as needed.\n",
        "\n",
        "Loading custom datasets that are not available in the TensorFlow Datasets library is one\n",
        "of the main advantages of using the `tf.data` pipeline. This approach allows you to\n",
        "create a custom data preprocessing pipeline tailored to the specific needs and\n",
        "requirements of your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggo8MEBW9vgo"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HHbmkEh9vgo"
      },
      "outputs": [],
      "source": [
        "SPLIT_RATIO = 0.2\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 5\n",
        "GLOBAL_CLIPNORM = 10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR8KEUtt9vgo"
      },
      "source": [
        "A dictionary is created to map each class name to a unique numerical identifier. This\n",
        "mapping is used to encode and decode the class labels during training and inference in\n",
        "object detection tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3XXhcIN9vgo"
      },
      "outputs": [],
      "source": [
        "class_ids = [\n",
        "    \"car\",\n",
        "    \"pedestrian\",\n",
        "    \"trafficLight\",\n",
        "    \"biker\",\n",
        "    \"truck\",\n",
        "]\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "\n",
        "# Path to images and annotations\n",
        "path_images = \"/kaggle/input/dataset/data/images/\"\n",
        "path_annot = \"/kaggle/input/dataset/data/annotations/\"\n",
        "\n",
        "# Get all XML file paths in path_annot and sort them\n",
        "xml_files = sorted(\n",
        "    [\n",
        "        os.path.join(path_annot, file_name)\n",
        "        for file_name in os.listdir(path_annot)\n",
        "        if file_name.endswith(\".xml\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get all JPEG image file paths in path_images and sort them\n",
        "jpg_files = sorted(\n",
        "    [\n",
        "        os.path.join(path_images, file_name)\n",
        "        for file_name in os.listdir(path_images)\n",
        "        if file_name.endswith(\".jpg\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7_BOO5w9vgo"
      },
      "source": [
        "The function below reads the XML file and finds the image name and path, and then\n",
        "iterates over each object in the XML file to extract the bounding box coordinates and\n",
        "class labels for each object.\n",
        "\n",
        "The function returns three values: the image path, a list of bounding boxes (each\n",
        "represented as a list of four floats: xmin, ymin, xmax, ymax), and a list of class IDs\n",
        "(represented as integers) corresponding to each bounding box. The class IDs are obtained\n",
        "by mapping the class labels to integer values using a dictionary called `class_mapping`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ix2L_Gw9vgo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_name = root.find(\"filename\").text\n",
        "    image_path = os.path.join(path_images, image_name)\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "    for obj in root.iter(\"object\"):\n",
        "        cls = obj.find(\"name\").text\n",
        "        classes.append(cls)\n",
        "\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = float(bbox.find(\"xmin\").text)\n",
        "        ymin = float(bbox.find(\"ymin\").text)\n",
        "        xmax = float(bbox.find(\"xmax\").text)\n",
        "        ymax = float(bbox.find(\"ymax\").text)\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    class_ids = [\n",
        "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
        "        for cls in classes\n",
        "    ]\n",
        "    return image_path, boxes, class_ids\n",
        "\n",
        "\n",
        "image_paths = []\n",
        "bbox = []\n",
        "classes = []\n",
        "for xml_file in tqdm(xml_files):\n",
        "    image_path, boxes, class_ids = parse_annotation(xml_file)\n",
        "    image_paths.append(image_path)\n",
        "    bbox.append(boxes)\n",
        "    classes.append(class_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDbKhckE9vgp"
      },
      "source": [
        "Here we are using `tf.ragged.constant` to create ragged tensors from the `bbox` and\n",
        "`classes` lists. A ragged tensor is a type of tensor that can handle varying lengths of\n",
        "data along one or more dimensions. This is useful when dealing with data that has\n",
        "variable-length sequences, such as text or time series data.\n",
        "\n",
        "```python\n",
        "classes = [\n",
        "    [8, 8, 8, 8, 8],      # 5 classes\n",
        "    [12, 14, 14, 14],     # 4 classes\n",
        "    [1],                  # 1 class\n",
        "    [7, 7],               # 2 classes\n",
        " ...]\n",
        "```\n",
        "\n",
        "```python\n",
        "bbox = [\n",
        "    [[199.0, 19.0, 390.0, 401.0],\n",
        "    [217.0, 15.0, 270.0, 157.0],\n",
        "    [393.0, 18.0, 432.0, 162.0],\n",
        "    [1.0, 15.0, 226.0, 276.0],\n",
        "    [19.0, 95.0, 458.0, 443.0]],     #image 1 has 4 objects\n",
        "    [[52.0, 117.0, 109.0, 177.0]],   #image 2 has 1 object\n",
        "    [[88.0, 87.0, 235.0, 322.0],\n",
        "    [113.0, 117.0, 218.0, 471.0]],   #image 3 has 2 objects\n",
        " ...]\n",
        "```\n",
        "\n",
        "In this case, the `bbox` and `classes` lists have different lengths for each image,\n",
        "depending on the number of objects in the image and the corresponding bounding boxes and\n",
        "classes. To handle this variability, ragged tensors are used instead of regular tensors.\n",
        "\n",
        "Later, these ragged tensors are used to create a `tf.data.Dataset` using the\n",
        "`from_tensor_slices` method. This method creates a dataset from the input tensors by\n",
        "slicing them along the first dimension. By using ragged tensors, the dataset can handle\n",
        "varying lengths of data for each image and provide a flexible input pipeline for further\n",
        "processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQoyFPH09vgp"
      },
      "outputs": [],
      "source": [
        "bbox = tf.ragged.constant(bbox)\n",
        "classes = tf.ragged.constant(classes)\n",
        "image_paths = tf.ragged.constant(image_paths)\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZL3ib19vgp"
      },
      "source": [
        "Splitting data in training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiKEdWTd9vgp"
      },
      "outputs": [],
      "source": [
        "# Determine the number of validation samples\n",
        "num_val = int(len(xml_files) * SPLIT_RATIO)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "val_data = data.take(num_val)\n",
        "train_data = data.skip(num_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NRu4NWq9vgp"
      },
      "source": [
        "Let's see about data loading and bounding box formatting to get things going. Bounding\n",
        "boxes in KerasCV have a predetermined format. To do this, you must bundle your bounding\n",
        "boxes into a dictionary that complies with the requirements listed below:\n",
        "\n",
        "```python\n",
        "bounding_boxes = {\n",
        "    # num_boxes may be a Ragged dimension\n",
        "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
        "    'classes': Tensor(shape=[batch, num_boxes])\n",
        "}\n",
        "```\n",
        "\n",
        "The dictionary has two keys, `'boxes'` and `'classes'`, each of which maps to a\n",
        "TensorFlow RaggedTensor or Tensor object. The `'boxes'` Tensor has a shape of `[batch,\n",
        "num_boxes, 4]`, where batch is the number of images in the batch and num_boxes is the\n",
        "maximum number of bounding boxes in any image. The 4 represents the four values needed to\n",
        "define a bounding box:  xmin, ymin, xmax, ymax.\n",
        "\n",
        "The `'classes'` Tensor has a shape of `[batch, num_boxes]`, where each element represents\n",
        "the class label for the corresponding bounding box in the `'boxes'` Tensor. The num_boxes\n",
        "dimension may be ragged, which means that the number of boxes may vary across images in\n",
        "the batch.\n",
        "\n",
        "Final dict should be:\n",
        "```python\n",
        "{\"images\": images, \"bounding_boxes\": bounding_boxes}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH5RhWEN9vgp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKuljxd9vgp"
      },
      "source": [
        "Here we create a layer that resizes images to 640x640 pixels, while maintaining the\n",
        "original aspect ratio. The bounding boxes associated with the image are specified in the\n",
        "`xyxy` format. If necessary, the resized image will be padded with zeros to maintain the\n",
        "original aspect ratio.\n",
        "\n",
        "Bounding Box Formats supported by KerasCV:\n",
        "1.   CENTER_XYWH\n",
        "2.   XYWH\n",
        "3.   XYXY\n",
        "4.   REL_XYXY\n",
        "5.   REL_XYWH\n",
        "6.   YXYX\n",
        "7.   REL_YXYX\n",
        "\n",
        "\n",
        "You can read more about KerasCV bounding box formats in\n",
        "[docs](https://keras.io/api/keras_cv/bounding_box/formats/).\n",
        "\n",
        "Furthermore, it is possible to perform format conversion between any two pairs:\n",
        "\n",
        "```python\n",
        "boxes = keras_cv.bounding_box.convert_format(\n",
        "        bounding_box,\n",
        "        images=image,\n",
        "        source=\"xyxy\",  # Original Format\n",
        "        target=\"xywh\",  # Target Format (to which we want to convert)\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IR2zDEf9vgp"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "One of the most challenging tasks when constructing object detection pipelines is data\n",
        "augmentation. It involves applying various transformations to the input images to\n",
        "increase the diversity of the training data and improve the model's ability to\n",
        "generalize. However, when working with object detection tasks, it becomes even more\n",
        "complex as these transformations need to be aware of the underlying bounding boxes and\n",
        "update them accordingly.\n",
        "\n",
        "KerasCV provides native support for bounding box augmentation. KerasCV offers an\n",
        "extensive collection of data augmentation layers specifically designed to handle bounding\n",
        "boxes. These layers intelligently adjust the bounding box coordinates as the image is\n",
        "transformed, ensuring that the bounding boxes remain accurate and aligned with the\n",
        "augmented images.\n",
        "\n",
        "By leveraging KerasCV's capabilities, developers can conveniently integrate bounding\n",
        "box-friendly data augmentation into their object detection pipelines. By performing\n",
        "on-the-fly augmentation within a tf.data pipeline, the process becomes seamless and\n",
        "efficient, enabling better training and more accurate object detection results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTAJ1Fvg9vgp"
      },
      "outputs": [],
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X53NKYSG9vgp"
      },
      "source": [
        "## Creating Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVEHbwRD9vgq"
      },
      "outputs": [],
      "source": [
        "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUvSvgyO9vgq"
      },
      "source": [
        "## Creating Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ncwija9vgq"
      },
      "outputs": [],
      "source": [
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(640, 640),\n",
        "    scale_factor=(0.75, 1.3),\n",
        "    bounding_box_format=\"xyxy\",\n",
        ")\n",
        "\n",
        "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKh1SFaw9vgq"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFH7qOKP9vgq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "    inputs = next(iter(inputs.take(1)))\n",
        "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=value_range,\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        y_true=bounding_boxes,\n",
        "        scale=5,\n",
        "        font_scale=0.7,\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_dataset(\n",
        "    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
        ")\n",
        "\n",
        "visualize_dataset(\n",
        "    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOpodWnO9vgq"
      },
      "source": [
        "We need to extract the inputs from the preprocessing dictionary and get them ready to be\n",
        "fed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haPcKl4t9vgq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZtYWl99vgq"
      },
      "source": [
        "## Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVYwmB7u9vgq"
      },
      "source": [
        "YOLOv8 is a cutting-edge YOLO model that is used for a variety of computer vision tasks,\n",
        "such as object detection, image classification, and instance segmentation. Ultralytics,\n",
        "the creators of YOLOv5, also developed YOLOv8, which incorporates many improvements and\n",
        "changes in architecture and developer experience compared to its predecessor. YOLOv8 is\n",
        "the latest state-of-the-art model that is highly regarded in the industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKmSWlts9vgq"
      },
      "source": [
        "Below table compares the performance metrics of five different YOLOv8 models with\n",
        "different sizes (measured in pixels): YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l, and YOLOv8x.\n",
        "The metrics include mean average precision (mAP) values at different\n",
        "intersection-over-union (IoU) thresholds for validation data, inference speed on CPU with\n",
        "ONNX format and A100 TensorRT, number of parameters, and number of floating-point\n",
        "operations (FLOPs) (both in millions and billions, respectively). As the size of the\n",
        "model increases, the mAP, parameters, and FLOPs generally increase while the speed\n",
        "decreases. YOLOv8x has the highest mAP, parameters, and FLOPs but also the slowest\n",
        "inference speed, while YOLOv8n has the smallest size, fastest inference speed, and lowest\n",
        "mAP, parameters, and FLOPs.\n",
        "\n",
        "| Model                                                                                |\n",
        "size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) |\n",
        "Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
        "| ------------------------------------------------------------------------------------ |\n",
        "--------------------- | -------------------- | ------------------------------ |\n",
        "----------------------------------- | ------------------ | ----------------- |\n",
        "| YOLOv8n | 640                   | 37.3                 | 80.4\n",
        "| 0.99                                | 3.2                | 8.7               |\n",
        "| YOLOv8s | 640                   | 44.9                 | 128.4\n",
        "| 1.20                                | 11.2               | 28.6              |\n",
        "| YOLOv8m | 640                   | 50.2                 | 234.7\n",
        "| 1.83                                | 25.9               | 78.9              |\n",
        "| YOLOv8l | 640                   | 52.9                 | 375.2\n",
        "| 2.39                                | 43.7               | 165.2             |\n",
        "| YOLOv8x | 640                   | 53.9                 | 479.1\n",
        "| 3.53                                | 68.2               | 257.8             |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-gz8hQv9vgq"
      },
      "source": [
        "You can read more about YOLOV8 and its architecture in this\n",
        "[RoboFlow Blog](https://blog.roboflow.com/whats-new-in-yolov8/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iikOZu4f9vgq"
      },
      "source": [
        "First we will create a instance of backbone which will be used by our yolov8 detector\n",
        "class.\n",
        "\n",
        "YOLOV8 Backbones available in KerasCV:\n",
        "\n",
        "1.   Without Weights:\n",
        "\n",
        "    1.   yolo_v8_xs_backbone\n",
        "    2.   yolo_v8_s_backbone\n",
        "    3.   yolo_v8_m_backbone\n",
        "    4.   yolo_v8_l_backbone\n",
        "    5.   yolo_v8_xl_backbone\n",
        "\n",
        "2. With Pre-trained coco weight:\n",
        "\n",
        "    1.   yolo_v8_xs_backbone_coco\n",
        "    2.   yolo_v8_s_backbone_coco\n",
        "    2.   yolo_v8_m_backbone_coco\n",
        "    2.   yolo_v8_l_backbone_coco\n",
        "    2.   yolo_v8_xl_backbone_coco\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l_FG3lG9vgq"
      },
      "outputs": [],
      "source": [
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
        "    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUdL89Z_9vgu"
      },
      "source": [
        "Next, let's build a YOLOV8 model using the `YOLOV8Detector`, which accepts a feature\n",
        "extractor as the `backbone` argument, a `num_classes` argument that specifies the number\n",
        "of object classes to detect based on the size of the `class_mapping` list, a\n",
        "`bounding_box_format` argument that informs the model of the format of the bbox in the\n",
        "dataset, and a finally, the feature pyramid network (FPN) depth is specified by the\n",
        "`fpn_depth` argument.\n",
        "\n",
        "It is simple to build a YOLOV8 using any of the aforementioned backbones thanks to\n",
        "KerasCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d0fxOA39vgu"
      },
      "outputs": [],
      "source": [
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdGB9Hnc9vgu"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDja5be9vgu"
      },
      "source": [
        "Loss used for YOLOV8\n",
        "\n",
        "\n",
        "1. Classification Loss: This loss function calculates the discrepancy between anticipated\n",
        "class probabilities and actual class probabilities. In this instance,\n",
        "`binary_crossentropy`, a prominent solution for binary classification issues, is\n",
        "Utilized. We Utilized binary crossentropy since each thing that is identified is either\n",
        "classed as belonging to or not belonging to a certain object class (such as a person, a\n",
        "car, etc.).\n",
        "\n",
        "2. Box Loss: `box_loss` is the loss function used to measure the difference between the\n",
        "predicted bounding boxes and the ground truth. In this case, the Complete IoU (CIoU)\n",
        "metric is used, which not only measures the overlap between predicted and ground truth\n",
        "bounding boxes but also considers the difference in aspect ratio, center distance, and\n",
        "box size. Together, these loss functions help optimize the model for object detection by\n",
        "minimizing the difference between the predicted and ground truth class probabilities and\n",
        "bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srCan0bg9vgu"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    global_clipnorm=GLOBAL_CLIPNORM,\n",
        ")\n",
        "\n",
        "yolo.compile(\n",
        "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmg0x4rN9vgu"
      },
      "source": [
        "## COCO Metric Callback\n",
        "\n",
        "We will be using `BoxCOCOMetrics` from KerasCV to evaluate the model and calculate the\n",
        "Map(Mean Average Precision) score, Recall and Precision. We also save our model when the\n",
        "mAP score improves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C48F0vhG9vgu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, data, save_path):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
        "            bounding_box_format=\"xyxy\",\n",
        "            evaluate_freq=1e9,\n",
        "        )\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.best_map = -1.0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.metrics.reset_state()\n",
        "        for batch in self.data:\n",
        "            images, y_true = batch[0], batch[1]\n",
        "            y_pred = self.model.predict(images, verbose=0)\n",
        "            self.metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        metrics = self.metrics.result(force=True)\n",
        "        logs.update(metrics)\n",
        "\n",
        "        current_map = metrics[\"MaP\"]\n",
        "        if current_map > self.best_map:\n",
        "            self.best_map = current_map\n",
        "            self.model.save(self.save_path)  # Save the model when mAP improves\n",
        "\n",
        "        return logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlPTTQHy9vgu"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7fqFHdG9vgu"
      },
      "outputs": [],
      "source": [
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3,\n",
        "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model.h5\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R__6dNAp9vgu"
      },
      "source": [
        "## Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_H-xxij9vgu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "yolov8",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}