{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+https://github.com/keras-team/keras-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b4mPR0JE1Zv",
    "outputId": "6e8612ba-28fa-445d-e5e4-a80b6449e0c0"
   },
   "outputs": [],
   "source": [
    "#!wget https://universe.roboflow.com/ds/GG1XJhlbDK?key=FFxqUqVjub -O indoor_object_detection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNwwp1WkJSBm",
    "outputId": "b5fd8bff-0eef-416e-e251-3372b18c623c"
   },
   "outputs": [],
   "source": [
    "#!unzip \"indoor_object_detection.zip\" -d \"dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYXAQXRv9vgn"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMTcKKGH9vgn",
    "outputId": "12f98969-478b-48ca-8188-1a836e169f74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:07:07.155894: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 15:07:07.280622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 15:07:07.280657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 15:07:07.290519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 15:07:07.327555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 15:07:08.130205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggo8MEBW9vgo"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6HHbmkEh9vgo"
   },
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 50\n",
    "GLOBAL_CLIPNORM = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i3XXhcIN9vgo"
   },
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"TV\", \"bed\", \"chair\", \"clock\", \"console\", \"consoleeeeee\", \"door\", \"fan\", \"light\", \"sofa\", \"switchboard\", \"table\"\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "# Path to images and annotations\n",
    "path_images = \"dataset/train/\"\n",
    "path_annot = \"dataset/train/\"\n",
    "\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_images, file_name)\n",
    "        for file_name in os.listdir(path_images)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2b949297991243388c2bbd0a04760f41",
      "a1c6cbfd5c614a68ba9a3bb1f5efc5f7",
      "611f30558a3d4278bb6cdeb52c98cff5",
      "23e4a255f1664c699d2d6cf00253559c",
      "6fc913a8f1604a639b5a45437af4718c",
      "e3c791bfcf6c453d85dae4f359b74f83",
      "66b3cacb326448829fc378be51cbeebe",
      "5dc5446c9611411ca61b12b715253919",
      "66722e0666bf4bfa96238cdec21491f6",
      "b618213856a34ec3830ad0b48a5a0028",
      "8c4cd4fca7d349d5b5d0eb212d5d84f2"
     ]
    },
    "id": "4ix2L_Gw9vgo",
    "outputId": "f93fe7ce-c63f-435a-8d18-47bfcff1898f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91384021e3c41deb532b99da5595136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_images, image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = float(bbox.find(\"xmin\").text)\n",
    "        ymin = float(bbox.find(\"ymin\").text)\n",
    "        xmax = float(bbox.find(\"xmax\").text)\n",
    "        ymax = float(bbox.find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "\n",
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(xml_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wQoyFPH09vgp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:07:31.499510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:31.586017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:31.586096: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:31.589244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:31.589276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:31.589291: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:32.914606: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:32.914652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:32.914658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-30 15:07:32.914680: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-30 15:07:32.916082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1590 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "bbox = tf.ragged.constant(bbox)\n",
    "classes = tf.ragged.constant(classes)\n",
    "image_paths = tf.ragged.constant(image_paths)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KiKEdWTd9vgp"
   },
   "outputs": [],
   "source": [
    "# Determine the number of validation samples\n",
    "num_val = int(len(xml_files) * SPLIT_RATIO)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "val_data = data.take(num_val)\n",
    "val_data = val_data.take(10)\n",
    "train_data = data.skip(num_val)\n",
    "train_data = train_data.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YI_j-84Jop52",
    "outputId": "1b103efb-b50f-494e-d510-0a05f311f651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4660\n",
      "932\n",
      "tf.Tensor(100, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(len(image_paths))\n",
    "print(num_val)\n",
    "print(train_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LH5RhWEN9vgp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bTAJ1Fvg9vgp"
   },
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.RandomShear(\n",
    "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X53NKYSG9vgp"
   },
   "source": [
    "## Creating Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CVEHbwRD9vgq"
   },
   "outputs": [],
   "source": [
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
    "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUvSvgyO9vgq"
   },
   "source": [
    "## Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "32ncwija9vgq"
   },
   "outputs": [],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(0.75, 1.3),\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
    "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKh1SFaw9vgq"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EFH7qOKP9vgq",
    "outputId": "a344eaca-46ad-45b5-b2e7-ae3dc160b099"
   },
   "outputs": [],
   "source": [
    "#import cv2\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "#visualize_dataset(\n",
    "#    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    "#)\n",
    "\n",
    "#visualize_dataset(\n",
    "#    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOpodWnO9vgq"
   },
   "source": [
    "We need to extract the inputs from the preprocessing dictionary and get them ready to be\n",
    "fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "haPcKl4t9vgq"
   },
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yZtYWl99vgq"
   },
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l_FG3lG9vgq",
    "outputId": "78070df1-0b4d-4da5-ceef-fd82e4b05294"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.10/site-packages/keras_cv/models/backbones/backbone.py:44: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
      "/home/andrea/.local/lib/python3.10/site-packages/keras_cv/models/backbones/backbone.py:44: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
     ]
    }
   ],
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_s_backbone_coco\",  # We will use yolov8 small backbone with coco weights\n",
    "    trainable = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-d0fxOA39vgu"
   },
   "outputs": [],
   "source": [
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdGB9Hnc9vgu"
   },
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "srCan0bg9vgu"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "#yolo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "C48F0vhG9vgu"
   },
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        #self.best_map = -1.0\n",
    "        self.best_map = 999\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        #print(metrics)\n",
    "        #current_map = metrics[\"MaP\"]\n",
    "        current_map = logs[\"box_loss\"]\n",
    "        if current_map < self.best_map:\n",
    "            self.best_map = current_map\n",
    "            now = datetime.now()\n",
    "            self.model.save(self.save_path + \"-ep\" + str(epoch) + now.strftime(\"-%m%d%Y-%H%M%S\") + \".h5\")\n",
    "\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sY6tk0O9MiAc"
   },
   "outputs": [],
   "source": [
    "yolo.load_weights(\"model-ep14-01292024-014637.h5\")\n",
    "#yolo = keras_cv.models.load(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo.save(\"model_3ep.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlPTTQHy9vgu"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7fqFHdG9vgu",
    "outputId": "13dd31b3-eb16-4cbd-8472-6a84a9a9d7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:10:01.437496: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-30 15:10:05.077780: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-01-30 15:10:09.220119: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-30 15:10:27.662797: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:27.793906: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:29.077359: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:29.206810: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:30.819861: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 140.59MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:30.974261: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 140.59MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:34.120131: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 550.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:34.268004: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 550.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:35.296945: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 560.59MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:35.443194: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 560.59MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-01-30 15:10:52.454619: I external/local_xla/xla/service/service.cc:168] XLA service 0x7faa56de1350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-30 15:10:52.454661: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-01-30 15:10:52.502637: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706623852.695617    6047 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - ETA: 0s - loss: 0.9558 - box_loss: 0.7854 - class_loss: 0.1704"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 15:11:20.686687: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 14745728 bytes after encountering the first element of size 14745728 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n",
      "2024-01-30 15:11:32.134322: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n",
      "    return func(device, token, args)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n",
      "    outputs = self._call(device, args)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n",
      "    ret = self._func(*args)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 205, in result_on_host_cpu\n",
      "    return tf.constant(obj_result(force), obj.dtype)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 256, in result\n",
      "    self._cached_result = self._compute_result()\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 264, in _compute_result\n",
      "    _box_concat(self.ground_truths),\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 44, in _box_concat\n",
      "    result[key] = tf.concat([b[key] for b in boxes], axis=0)\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "\n",
      "  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n",
      "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
      "\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__EagerPyFunc_Tin_1_Tout_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\nTraceback (most recent call last):\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 205, in result_on_host_cpu\n    return tf.constant(obj_result(force), obj.dtype)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 256, in result\n    self._cached_result = self._compute_result()\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 264, in _compute_result\n    _box_concat(self.ground_truths),\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 44, in _box_concat\n    result[key] = tf.concat([b[key] for b in boxes], axis=0)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\n\n [Op:EagerPyFunc] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43myolo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEvaluateCOCOMetricsCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m, in \u001b[0;36mEvaluateCOCOMetricsCallback.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(images, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate_state(y_true, y_pred)\n\u001b[0;32m---> 21\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m logs\u001b[38;5;241m.\u001b[39mupdate(metrics)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(metrics)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#current_map = metrics[\"MaP\"]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py:210\u001b[0m, in \u001b[0;36mBoxCOCOMetrics.__new__.<locals>.result_fn\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 210\u001b[0m     py_func_result \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_on_host_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(METRIC_NAMES):\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__EagerPyFunc_Tin_1_Tout_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\nTraceback (most recent call last):\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 205, in result_on_host_cpu\n    return tf.constant(obj_result(force), obj.dtype)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 256, in result\n    self._cached_result = self._compute_result()\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 264, in _compute_result\n    _box_concat(self.ground_truths),\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/keras_cv/metrics/object_detection/box_coco_metrics.py\", line 44, in _box_concat\n    result[key] = tf.concat([b[key] for b in boxes], axis=0)\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/home/andrea/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [3,3,4] vs. shape[2] = [3,2,4] [Op:ConcatV2] name: concat\n\n [Op:EagerPyFunc] name: "
     ]
    }
   ],
   "source": [
    "yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23e4a255f1664c699d2d6cf00253559c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b618213856a34ec3830ad0b48a5a0028",
      "placeholder": "​",
      "style": "IPY_MODEL_8c4cd4fca7d349d5b5d0eb212d5d84f2",
      "value": " 4660/4660 [00:03&lt;00:00, 1138.91it/s]"
     }
    },
    "2b949297991243388c2bbd0a04760f41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a1c6cbfd5c614a68ba9a3bb1f5efc5f7",
       "IPY_MODEL_611f30558a3d4278bb6cdeb52c98cff5",
       "IPY_MODEL_23e4a255f1664c699d2d6cf00253559c"
      ],
      "layout": "IPY_MODEL_6fc913a8f1604a639b5a45437af4718c"
     }
    },
    "5dc5446c9611411ca61b12b715253919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "611f30558a3d4278bb6cdeb52c98cff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dc5446c9611411ca61b12b715253919",
      "max": 4660,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66722e0666bf4bfa96238cdec21491f6",
      "value": 4660
     }
    },
    "66722e0666bf4bfa96238cdec21491f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66b3cacb326448829fc378be51cbeebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc913a8f1604a639b5a45437af4718c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c4cd4fca7d349d5b5d0eb212d5d84f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c6cbfd5c614a68ba9a3bb1f5efc5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3c791bfcf6c453d85dae4f359b74f83",
      "placeholder": "​",
      "style": "IPY_MODEL_66b3cacb326448829fc378be51cbeebe",
      "value": "100%"
     }
    },
    "b618213856a34ec3830ad0b48a5a0028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3c791bfcf6c453d85dae4f359b74f83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
