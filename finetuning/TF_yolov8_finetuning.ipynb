{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+https://github.com/keras-team/keras-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade keras-cv\n",
    "#!pip install -q --upgrade keras  # Upgrade to Keras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNwwp1WkJSBm",
    "outputId": "b5fd8bff-0eef-416e-e251-3372b18c623c"
   },
   "outputs": [],
   "source": [
    "#!wget \"https://universe.roboflow.com/ds/UZFvFHcTp4?key=Ba0HgFQwHR\" -O indoor_object_detection.zip\n",
    "#!unzip \"indoor_object_detection.zip\" -d \"dataset3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://universe.roboflow.com/ds/GG1XJhlbDK?key=FFxqUqVjub -O indoor_object_detection.zip\n",
    "#!unzip \"indoor_object_detection.zip\" -d \"dataset1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYXAQXRv9vgn"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMTcKKGH9vgn",
    "outputId": "12f98969-478b-48ca-8188-1a836e169f74"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_cv\n",
    "from keras_cv import visualization\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv.layers import Mosaic, Resizing\n",
    "import datetime\n",
    "import pickle\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import cv2\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per bypassare messaggi di warning relativi all'implementazione dei layer di Data-Augmentation\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# impostazione per TensorFlow in modo che non allochi tutta\n",
    "# la memoria della GPU ma solo quella necessaria, in modo da\n",
    "# poter lanciare altri addestramenti in parallelo a questo\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for pd in physical_devices:\n",
    "   try:\n",
    "     tf.config.experimental.set_memory_growth(pd, True)\n",
    "   except:\n",
    "     # device non valida/compatibile\n",
    "     pass"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gpu_available = tf.test.is_gpu_available()\n",
    "print(\"gpu_available\", gpu_available)\n",
    "\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "print(\"is_cuda_gpu_available\", is_cuda_gpu_available)\n",
    "\n",
    "is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))\n",
    "print(\"is_cuda_gpu_min_3\", is_cuda_gpu_min_3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Percorso del DataSet\n",
    "\n",
    "Il Dataset deve essere nel formato PascalVOC e deve essere strutturato nelle seguenti cartelle\n",
    "\n",
    "cartella principale dove trovare il dataset\n",
    "PATH_DATASET = \"../../Yolo8_Dataset\"\n",
    "\n",
    "nome del dataset che deve corrisponedere alla cartella fisica presente nella cartella principale\n",
    "DATASET_NAME = \"animali\"\n",
    "\n",
    "devono essere presenti infine la cartella di train e di test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PATH_DATASET = \"../../Yolo8_Dataset\"\n",
    "\n",
    "DATASET_NAME = \"animali\"\n",
    "TRAIN_DIR_NAME = \"train\"\n",
    "VALID_DIR_NAME = \"test\"\n",
    "\n",
    "\n",
    "PATH_TRAIN = PATH_DATASET + \"/\" + DATASET_NAME + \"/\" + TRAIN_DIR_NAME + \"/\"\n",
    "PATH_VALID = PATH_DATASET + \"/\" + DATASET_NAME + \"/\" + VALID_DIR_NAME + \"/\"\n",
    "\n",
    "PATH_TRAIN_MOSAIC = PATH_DATASET + \"/\" + DATASET_NAME + \"/train_mosaic/\"\n",
    "PATH_VALID_MOSAIC = PATH_DATASET + \"/\" + DATASET_NAME + \"/valid_mosaic/\"\n",
    "\n",
    "\n",
    "PATH_MODELS = PATH_DATASET + \"/\" + DATASET_NAME + \"/models/\"\n",
    "PATH_LOG_TENSORBOARD = PATH_DATASET + \"/logs\"\n",
    "\n",
    "\n",
    "if not(os.path.exists(PATH_VALID_MOSAIC)):\n",
    "    os.makedirs(PATH_VALID_MOSAIC)\n",
    "    \n",
    "if not(os.path.exists(PATH_TRAIN_MOSAIC)):\n",
    "    os.makedirs(PATH_TRAIN_MOSAIC)\n",
    "    \n",
    "if not(os.path.exists(PATH_MODELS)):\n",
    "    os.makedirs(PATH_MODELS)\n",
    "    \n",
    "if not(os.path.exists(PATH_LOG_TENSORBOARD)):\n",
    "    os.makedirs(PATH_LOG_TENSORBOARD)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tot_train = len([file_name for file_name in os.listdir(PATH_TRAIN) if file_name.endswith(\".xml\")])\n",
    "tot_valid = len([file_name for file_name in os.listdir(PATH_VALID) if file_name.endswith(\".xml\")])\n",
    "\n",
    "print(F\"Tot samples train: {tot_train}\")\n",
    "print(F\"Tot samples valid: {tot_valid}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggo8MEBW9vgo"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HHbmkEh9vgo"
   },
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1.000e-03\n",
    "EPOCH = 100\n",
    "INITIAL_EPOCH = 0\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "bestResult = -1 # best MaP of previous session (-1 for new session)\n",
    "\n",
    "BACKBONE_SIZE = \"m\"\n",
    "BACKBONE_NAME =\"yolo_v8_\" + BACKBONE_SIZE + \"_backbone_coco\"\n",
    "BACKBONE_TRAINABLE = False\n",
    "BACKBONE_UNFREEZE = False\n",
    "USE_AUGMENTATION = True\n",
    "CREATE_MOSAIC = False\n",
    "USE_MOSAIC = True\n",
    "USE_MIXED = True\n",
    "EPOCH_MIN_SAVE = 0\n",
    "\n",
    "MODEL_NAME = \"{dataset}-model-{size}{freezed}{mosaic}{mixed}\".format(mosaic=\"-mosaic\" if USE_MOSAIC == True else \"\", mixed=\"-mixed\" if USE_MIXED == True else \"\",size=BACKBONE_SIZE, freezed= \"-bbfreez\" if BACKBONE_TRAINABLE == False else \"\", dataset=DATASET_NAME)\n",
    "HISTORY_NAME = MODEL_NAME + \"-history\"\n",
    "\n",
    "#load previous training best result\n",
    "if os.path.isfile(PATH_MODELS + MODEL_NAME + \".pkl\"):\n",
    "    with open(PATH_MODELS + MODEL_NAME + \".pkl\", 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "        bestResult = loaded_dict[\"BestMap\"].numpy()\n",
    "print(F\"using best MaP: {bestResult}\")\n",
    "\n",
    "# elements to use for debug callbacks (use 0 in real train process)\n",
    "debug_size_train = 0\n",
    "debug_size_valid = debug_size_train * SPLIT_RATIO\n",
    "\n",
    "class_ids = [\n",
    "    #\"TV\", \"bed\", \"chair\", \"clock\", \"console\", \"consoleeeeee\", \"door\", \"fan\", \"light\", \"sofa\", \"switchboard\", \"table\"\n",
    "    #\"emptychair\", \"fullchair\"\n",
    "    #\"Chair\",\"Sofa\",\"Table\"\n",
    "    \"Cane\", \"Gatto\"\n",
    "]\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3XXhcIN9vgo"
   },
   "outputs": [],
   "source": [
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "def GetDataFiles(path, filter_exclude=None, filter_include=None) :\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "    xml_files = sorted(\n",
    "        [\n",
    "            os.path.join(path, file_name)\n",
    "            for file_name in os.listdir(path)\n",
    "            if file_name.endswith(\".xml\") and (not file_name.startswith(filter_exclude) if filter_exclude is not None else True) and (file_name.startswith(filter_include) if filter_include is not None else True)\n",
    "        ]\n",
    "    )\n",
    "    return xml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2b949297991243388c2bbd0a04760f41",
      "a1c6cbfd5c614a68ba9a3bb1f5efc5f7",
      "611f30558a3d4278bb6cdeb52c98cff5",
      "23e4a255f1664c699d2d6cf00253559c",
      "6fc913a8f1604a639b5a45437af4718c",
      "e3c791bfcf6c453d85dae4f359b74f83",
      "66b3cacb326448829fc378be51cbeebe",
      "5dc5446c9611411ca61b12b715253919",
      "66722e0666bf4bfa96238cdec21491f6",
      "b618213856a34ec3830ad0b48a5a0028",
      "8c4cd4fca7d349d5b5d0eb212d5d84f2"
     ]
    },
    "id": "4ix2L_Gw9vgo",
    "outputId": "f93fe7ce-c63f-435a-8d18-47bfcff1898f"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_annotation(xml_file, path_images):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_images, image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = float(bbox.find(\"xmin\").text)\n",
    "        ymin = float(bbox.find(\"ymin\").text)\n",
    "        xmax = float(bbox.find(\"xmax\").text)\n",
    "        ymax = float(bbox.find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "def create_object_arrays(xml_files, path, num_examples=0) :\n",
    "    image_paths = []\n",
    "    bbox = []\n",
    "    classes = []\n",
    "    i = 0\n",
    "    for xml_file in tqdm(xml_files):\n",
    "        i = i+1\n",
    "        if num_examples > 0 and i > num_examples :\n",
    "            break            \n",
    "        image_path, boxes, class_ids = parse_annotation(xml_file, path)\n",
    "        image_paths.append(image_path)\n",
    "        bbox.append(boxes)\n",
    "        classes.append(class_ids)\n",
    "\n",
    "    return image_paths, bbox, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQoyFPH09vgp"
   },
   "outputs": [],
   "source": [
    "def get_data(path, num_examples=0, filter_exclude=None, filter_include=None):\n",
    "\n",
    "\n",
    "    xmls = GetDataFiles(path, filter_exclude, filter_include)\n",
    "    image_paths, bbox, classes = create_object_arrays(xmls, path, num_examples)\n",
    "\n",
    "    bbox = tf.ragged.constant(bbox)\n",
    "    classes = tf.ragged.constant(classes)\n",
    "    image_paths = tf.ragged.constant(image_paths)\n",
    "\n",
    "    return  tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH5RhWEN9vgp"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreaFilePascalVOC(path_salvataggio, nome_file, immagine_mosaico, immagine_bbox, immagine_label):\n",
    "    \n",
    "    NomeFileImmagine = path_salvataggio + nome_file \n",
    "    \n",
    "    AltezzaImmagine, LarghezzaImmagine, CanaliImmagine = immagine_mosaico.shape[:3]\n",
    "    immagine_mosaico = cv2.cvtColor(immagine_mosaico, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(NomeFileImmagine + \".jpg\", immagine_mosaico)\n",
    "    \n",
    "    #Inizializzo il file Xml\n",
    "    xmlRoot = ET.Element(\"annotation\")\n",
    "    \n",
    "    xmlfolder = ET.Element(\"folder\")\n",
    "    xmlRoot.append(xmlfolder)\n",
    "    \n",
    "    xmlfilename = ET.Element(\"filename\")\n",
    "    xmlfilename.text = str(nome_file) + \".jpg\"\n",
    "    xmlRoot.append(xmlfilename)\n",
    "    \n",
    "    xmlpath = ET.Element(\"path\")\n",
    "    xmlpath.text = str(NomeFileImmagine) + \".jpg\"\n",
    "    xmlRoot.append(xmlpath)\n",
    "    \n",
    "    xmlsource = ET.Element(\"source\")\n",
    "    xmldatabase = ET.Element(\"database\")\n",
    "    xmldatabase.text = \"EasyManagement.it\"\n",
    "    xmlsource.append(xmldatabase)\n",
    "    xmlRoot.append(xmlsource)\n",
    "    \n",
    "    xmlsize = ET.Element(\"size\")\n",
    "    xmlwidht = ET.Element(\"width\")\n",
    "    xmlwidht.text = str(LarghezzaImmagine)\n",
    "    xmlheight = ET.Element(\"height\")\n",
    "    xmlheight.text = str(AltezzaImmagine)\n",
    "    xmldepth = ET.Element(\"depth\")\n",
    "    xmldepth.text = str(CanaliImmagine)\n",
    "    xmlsize.append(xmlwidht)\n",
    "    xmlsize.append(xmlheight)\n",
    "    xmlsize.append((xmldepth))\n",
    "    xmlRoot.append(xmlsize)\n",
    "    \n",
    "    xmlsegmented = ET.Element(\"segmented\")\n",
    "    xmlsegmented.text = \"0\"\n",
    "    xmlRoot.append(xmlsegmented)\n",
    "    \n",
    "    for k in range(0, len(immagine_bbox)):\n",
    "        annotazione = immagine_bbox[k]\n",
    "        classe = immagine_label[k]\n",
    "        \n",
    "        xmlobject = ET.Element(\"object\")\n",
    "        xmlobject_name = ET.Element(\"name\")\n",
    "        xmlobject_name.text = class_ids[classe]\n",
    "        xmlobject.append(xmlobject_name)\n",
    "\n",
    "        xmlobject_pose = ET.Element(\"pose\")\n",
    "        xmlobject_pose.text = \"Unspecified\"\n",
    "        xmlobject.append(xmlobject_pose)\n",
    "\n",
    "        xmlobject_truncated = ET.Element(\"truncated\")\n",
    "        xmlobject_truncated.text = \"0\"\n",
    "        xmlobject.append(xmlobject_truncated)\n",
    "\n",
    "        xmlobject_difficult = ET.Element(\"difficult\")\n",
    "        xmlobject_difficult.text = \"0\"\n",
    "        xmlobject.append(xmlobject_difficult)\n",
    "\n",
    "        xmlobject_occluded = ET.Element(\"occluded\")\n",
    "        xmlobject_occluded.text = \"0\"\n",
    "        xmlobject.append(xmlobject_occluded)\n",
    "\n",
    "        xmlobject_bbox = ET.Element(\"bndbox\")\n",
    "        xmlobject_bbox_xmin = ET.Element(\"xmin\")\n",
    "        xmlobject_bbox_xmin.text = str(immagine_bbox[k][0])\n",
    "        xmlobject_bbox_xmax = ET.Element(\"ymin\")\n",
    "        xmlobject_bbox_xmax.text = str(immagine_bbox[k][1])\n",
    "        xmlobject_bbox_ymin = ET.Element(\"xmax\")\n",
    "        xmlobject_bbox_ymin.text = str(immagine_bbox[k][2])\n",
    "        xmlobject_bbox_ymax = ET.Element(\"ymax\")\n",
    "        xmlobject_bbox_ymax.text = str(immagine_bbox[k][3])\n",
    "\n",
    "        xmlobject_bbox.append(xmlobject_bbox_xmin)\n",
    "        xmlobject_bbox.append(xmlobject_bbox_xmax)\n",
    "        xmlobject_bbox.append(xmlobject_bbox_ymin)\n",
    "        xmlobject_bbox.append(xmlobject_bbox_ymax)\n",
    "        xmlobject.append(xmlobject_bbox)\n",
    "\n",
    "        xmlRoot.append(xmlobject)\n",
    "    \n",
    "\n",
    "    xmlTree = ET.ElementTree(xmlRoot)\n",
    "    ET.indent(xmlTree,'    ')\n",
    "    xmlTree.write(NomeFileImmagine + \".xml\", xml_declaration=True, encoding='utf-8', default_namespace=None, method='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter_mosaic = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.Resizing(height=640, width=640, pad_to_aspect_ratio=True, bounding_box_format=\"xyxy\"),\n",
    "        #keras_cv.layers.RandomShear(x_factor=(0.,0.2), y_factor=(0.,0.2), bounding_box_format=\"xyxy\",fill_mode=\"nearest\"),\n",
    "        keras_cv.layers.RandomHue(factor=0.015,value_range=(0., 255.)),\n",
    "        keras_cv.layers.RandomBrightness(factor=0.25,value_range=(0., 255.)),\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.RandomRotation(factor=(-0.2, 0.2), bounding_box_format=\"xyxy\", fill_mode=\"nearest\"),\n",
    "        keras_cv.layers.JitteredResize(target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTAJ1Fvg9vgp"
   },
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomHue(factor=0.015,value_range=(0., 255.)),\n",
    "        keras_cv.layers.RandomBrightness(factor=0.25,value_range=(0., 255.)),\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.RandomShear(x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.JitteredResize(target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmenter_none = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.JitteredResize(target_size=(640, 640), scale_factor=(1., 1.), bounding_box_format=\"xyxy\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mosaic(all_img_list, all_annos, all_classes, idxs, output_size, mosaic_size):\n",
    "    # Create an empty canvas for the output image\n",
    "    output_img = np.zeros([output_size[0], output_size[1], 3], dtype=np.uint8)\n",
    "    \n",
    "    rows=mosaic_size[0]\n",
    "    cols=mosaic_size[1]\n",
    "    \n",
    "    # Calculate the scale based on the selected mosaic size\n",
    "    scale_x = int(output_size[1]/cols)\n",
    "    scale_y = int(output_size[0]/rows)\n",
    "\n",
    "    # Initialize a list for new annotations\n",
    "    new_anno = []\n",
    "    new_classes = []\n",
    "    # Process each index and its respective image\n",
    "    idx=0\n",
    "    r=0\n",
    "    c=0\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            #path = all_img_list[idx]  # Image path\n",
    "            img_annos = all_annos[idx]  # Image annotations\n",
    "            img_classes = all_classes[idx]\n",
    "            #img = cv2.imread(path)  # Read the image\n",
    "            img = all_img_list[idx]\n",
    "            \n",
    "            ratio_x = scale_x/img.shape[1]\n",
    "            ratio_y = scale_y/img.shape[0]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (scale_x, scale_y))\n",
    "            img = img[...,::-1]\n",
    "            output_img[scale_y*r:scale_y+scale_y*r, scale_x*c:scale_x+scale_x*c, :] = img\n",
    "            class_idx = 0 \n",
    "            for bbox in img_annos:  # Update annotations accordingly\n",
    "                xmin = bbox[0]*ratio_x+scale_x*c\n",
    "                ymin = bbox[1]*ratio_y+scale_y*r\n",
    "                xmax = bbox[2]*ratio_x+scale_x*c\n",
    "                ymax = bbox[3]*ratio_y+scale_y*r\n",
    "                new_anno.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "                new_classes.append(img_classes[class_idx])\n",
    "                \n",
    "                class_idx += 1\n",
    "            idx+=1\n",
    "            c+=1\n",
    "        r+=1\n",
    "\n",
    "    return output_img, new_anno, new_classes # Return the generated mosaic image and its annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_mosaic(image_path, save_path, n_max_image=None, perc_of_tot=None, mosaic_size=(2,2), copy_val=False, copy_test=False, save_path_val=None, save_path_test=None, verbous=False, show_preview=False):\n",
    "    \n",
    "    xml_files = GetDataFiles(image_path, filter_exclude=\"augment_mosaic_\")\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(xml_files)\n",
    "    n_mosaic_images = mosaic_size[0] * mosaic_size[1]\n",
    "    tot_images = len(xml_files)\n",
    "\n",
    "    data_mosaic = get_data(image_path, debug_size_train, filter_exclude=\"augment_mosaic_\")\n",
    "    mosaic_ds = data_mosaic.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    mosaic_ds = mosaic_ds.shuffle(tot_images)\n",
    "    mosaic_ds = mosaic_ds.ragged_batch(1, drop_remainder=True)\n",
    "    mosaic_ds = mosaic_ds.map(augmenter_mosaic, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    idx=0\n",
    "    if n_max_image is None or n_max_image > tot_images/n_mosaic_images :\n",
    "        n_max_image= int(tot_images/n_mosaic_images)\n",
    "        print(F\"Max number of generateable images has been reset to {n_max_image}\")\n",
    "\n",
    "    if perc_of_tot is not None:\n",
    "        n_max_image = int((tot_images*perc_of_tot/100)/n_mosaic_images)\n",
    "        \n",
    "    for n_mosaic in range(0, n_max_image):        \n",
    "        print(F\"\\rprocess image {n_mosaic+1 }/{n_max_image}\", end=\"\")\n",
    "\n",
    "        mosiac_image_list = []\n",
    "        mosiac_bbox_list = []\n",
    "        mosiac_bbox_class = []\n",
    "        fig = None\n",
    "        \n",
    "        if show_preview==True:\n",
    "            fig = plt.figure(figsize=(10,7)) \n",
    "        y=0\n",
    "        for item in mosaic_ds.skip(idx).take(n_mosaic_images):\n",
    "            idx = n_mosaic+y\n",
    "\n",
    "            img_orig =item[\"images\"][0].numpy().astype(\"uint8\")\n",
    "            img=img_orig.copy()\n",
    "            \n",
    "            boxes = item[\"bounding_boxes\"][\"boxes\"][0].numpy().astype(int)\n",
    "            class_id = item[\"bounding_boxes\"][\"classes\"][0].numpy().astype(int)\n",
    "            boxes=list(boxes)\n",
    "            class_id=list(class_id)\n",
    "\n",
    "            imarray = np.random.rand(640,640,3) * 255\n",
    "            dummy_img = imarray.astype('uint8')\n",
    "\n",
    "            mosiac_image_list.append(img if len(boxes) > 0 else dummy_img)\n",
    "            mosiac_bbox_list.append(boxes if len(boxes) > 0 else [])\n",
    "            mosiac_bbox_class.append(class_id if len(boxes) > 0 else [])\n",
    "\n",
    "            if show_preview==True:\n",
    "                i=0\n",
    "                for ann in boxes:\n",
    "                    color =  (255, 0, 0) if class_id[i] == 0 else  (255, 255, 0) \n",
    "                    cv2.rectangle(img_orig, (int(ann[0]), int(ann[1])), (int(ann[2]), int(ann[3])), color, 2 )\n",
    "                    i+=1\n",
    "                \n",
    "                fig.add_subplot(1, n_mosaic_images+1, (y+1)) \n",
    "                plt.title(F\"Orig {y+1}\")\n",
    "                plt.axis('off') \n",
    "                plt.imshow(img_orig)\n",
    "                if verbous==True:\n",
    "                    print(F\"boxes: \\r {boxes}\")\n",
    "            y+=1\n",
    "\n",
    "        idxs = np.arange(n_mosaic_images)\n",
    "\n",
    "        output_size = (640, 640)  # Dimensions of the final mosaic image\n",
    "\n",
    "        if verbous==True:\n",
    "            # Debugging - Print out values for inspection\n",
    "            print(\"Number of images:\", len(mosiac_image_list))\n",
    "            print(\"Number of annotations:\", len(mosiac_bbox_list))\n",
    "            print(\"Indices for mosaic:\", idxs)\n",
    "\n",
    "        # Call the mosaic function\n",
    "        mosaic_img, updated_annotations, updated_classes = create_mosaic(mosiac_image_list, mosiac_bbox_list, mosiac_bbox_class, idxs, output_size, mosaic_size)\n",
    "\n",
    "        if show_preview==True:\n",
    "            i=0\n",
    "            for ann in updated_annotations:\n",
    "                color =  (255, 0, 0) if updated_classes[i] == 0 else  (255, 255, 0) \n",
    "                cv2.rectangle(mosaic_img, (ann[0], ann[1]), (ann[2], ann[3]), color, 2 )\n",
    "                i+=1\n",
    "            \n",
    "            fig = plt.figure() \n",
    "            plt.axis('off') \n",
    "            plt.title(\"mosaic\")\n",
    "            plt.imshow(mosaic_img)\n",
    "            #plt.imshow(mosaic_img)\n",
    "\n",
    "        if verbous==True:\n",
    "            # Access and use the updated_annotations for further processing\n",
    "            print(\"Updated Annotations:\")\n",
    "            print(updated_annotations)\n",
    "\n",
    "        #plt.close()\n",
    "        if show_preview==False:        \n",
    "            CreaFilePascalVOC(save_path, F\"augment_mosaic_{uuid.uuid4().hex}\", mosaic_img, updated_annotations, updated_classes)\n",
    "            if copy_val == True:\n",
    "                CreaFilePascalVOC(save_path_val, F\"augment_mosaic_{uuid.uuid4().hex}\", mosaic_img, updated_annotations, updated_classes)\n",
    "            if copy_test == True:\n",
    "                CreaFilePascalVOC(save_path_test, F\"augment_mosaic_{uuid.uuid4().hex}\", mosaic_img, updated_annotations, updated_classes)\n",
    "\n",
    "    print(F\"\\rcreated {n_max_image} mosaic images (with {n_max_image*n_mosaic_images} images)\")\n",
    "    return n_max_image*n_mosaic_images"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if CREATE_MOSAIC == True:\n",
    "   for size in [(2,2)]:\n",
    "      augment_mosaic(PATH_TRAIN, PATH_TRAIN_MOSAIC, n_max_image=5, copy_val=False, mosaic_size=size, verbous=False, show_preview=True, save_path_val=PATH_VALID_MOSAIC)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_MOSAIC == True:\n",
    "    img_to_gen = 300 #tot_train * 2 #-478  #images already exist\n",
    "    img_gen = 0\n",
    "    while img_gen < img_to_gen:\n",
    "        for size in [(2,2),(2,3),(3,3),(3,4),(4,4)]:\n",
    "            n_img_val = int(tot_train/(size[0]*size[1])*0.1)\n",
    "            img_gen += augment_mosaic(PATH_TRAIN, PATH_TRAIN_MOSAIC, n_max_image=None, copy_val=False, mosaic_size=size, verbous=False, show_preview=False, save_path_val=PATH_VALID_MOSAIC)\n",
    "            img_gen += augment_mosaic(PATH_TRAIN, PATH_TRAIN_MOSAIC, n_max_image=n_img_val, copy_val=False, mosaic_size=size, verbous=False, show_preview=False, save_path_val=PATH_VALID_MOSAIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiKEdWTd9vgp"
   },
   "outputs": [],
   "source": [
    "train_data = get_data(PATH_TRAIN, debug_size_train)\n",
    "val_data = get_data(PATH_VALID, debug_size_valid)\n",
    "print(train_data.__len__())\n",
    "print(val_data.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X53NKYSG9vgp"
   },
   "source": [
    "## Creating Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVEHbwRD9vgq"
   },
   "outputs": [],
   "source": [
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(train_data.__len__())\n",
    "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = train_ds.map(augmenter if USE_AUGMENTATION==True else augmenter_none, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUvSvgyO9vgq"
   },
   "source": [
    "## Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32ncwija9vgq"
   },
   "outputs": [],
   "source": [
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(val_data.__len__())\n",
    "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_ds = val_ds.map(augmenter if USE_AUGMENTATION==True else augmenter_none, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mosaic Dataset or Mixed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_MOSAIC == True or USE_MIXED == True:\n",
    "    train_data_mosaic = get_data(PATH_TRAIN_MOSAIC, debug_size_train)\n",
    "    val_data_mosaic = get_data(PATH_VALID_MOSAIC, debug_size_valid)\n",
    "    print(train_data_mosaic.__len__())\n",
    "    print(val_data_mosaic.__len__())\n",
    "    \n",
    "    train_mosaic_ds = train_data_mosaic.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_mosaic_ds = train_mosaic_ds.shuffle(train_data_mosaic.__len__())\n",
    "    train_mosaic_ds = train_mosaic_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    train_mosaic_ds = train_mosaic_ds.map(augmenter_none, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if val_data_mosaic.__len__()>0:\n",
    "        val_mosaic_ds = val_data_mosaic.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        val_mosaic_ds = val_mosaic_ds.shuffle(val_data_mosaic.__len__())\n",
    "        val_mosaic_ds = val_mosaic_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "        val_mosaic_ds = val_mosaic_ds.map(augmenter_none, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if USE_MIXED == True:\n",
    "        def align_ds(inputs):\n",
    "            classes = tf.cast(inputs[\"bounding_boxes\"][\"classes\"], dtype=tf.float32)\n",
    "            boxes = tf.cast(inputs[\"bounding_boxes\"][\"boxes\"], dtype=tf.float32)\n",
    "            inputs[\"bounding_boxes\"] = {\"classes\":classes, \"boxes\":boxes}\n",
    "            return {\"images\":inputs[\"images\"], \"bounding_boxes\":inputs[\"bounding_boxes\"]}\n",
    "\n",
    "        _train_ds = train_ds.map(align_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        _train_mosaic_ds = train_mosaic_ds.map(align_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        _val_ds = val_ds.map(align_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if val_data_mosaic.__len__()>0:\n",
    "            _val_mosaic_ds = val_mosaic_ds.map(align_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        train_mix_ds = _train_ds.concatenate(_train_mosaic_ds)\n",
    "        if val_data_mosaic.__len__()>0:\n",
    "            val_mix_ds = _val_ds.concatenate(_val_mosaic_ds)\n",
    "        else:\n",
    "            val_mix_ds = _val_ds\n",
    "\n",
    "        train_ds = train_mix_ds\n",
    "        val_ds = val_mix_ds\n",
    "    else:\n",
    "        train_ds = train_mosaic_ds\n",
    "        val_ds = val_mosaic_ds\n",
    "        \n",
    "        \n",
    "    print(train_ds.__len__())\n",
    "    print(val_ds.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    \n",
    "    \n",
    "    inputs = next(iter(inputs.take(10)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.shuffle(train_ds.__len__())\n",
    "val_ds = val_ds.shuffle(train_ds.__len__())\n",
    "\n",
    "visualize_dataset(\n",
    "    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")\n",
    "\n",
    "visualize_dataset(\n",
    "    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haPcKl4t9vgq"
   },
   "outputs": [],
   "source": [
    "# def dict_to_tuple(inputs):\n",
    "#     return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n",
    "#         inputs[\"bounding_boxes\"], max_boxes=32\n",
    "#     )\n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yZtYWl99vgq"
   },
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l_FG3lG9vgq",
    "outputId": "78070df1-0b4d-4da5-ceef-fd82e4b05294"
   },
   "outputs": [],
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    BACKBONE_NAME,  # We will use yolov8 small backbone with coco weights\n",
    "    load_weights=True\n",
    ")\n",
    "backbone.trainable = BACKBONE_TRAINABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d0fxOA39vgu"
   },
   "outputs": [],
   "source": [
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=backbone,\n",
    "    #fpn_depth=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdGB9Hnc9vgu"
   },
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY6tk0O9MiAc"
   },
   "outputs": [],
   "source": [
    "print(PATH_MODELS + MODEL_NAME)\n",
    "if os.path.isfile(PATH_MODELS + MODEL_NAME + \".h5\"):\n",
    "    yolo.load_weights(PATH_MODELS + MODEL_NAME + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srCan0bg9vgu"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n",
    "\n",
    "if BACKBONE_UNFREEZE == True:\n",
    "    backbone.trainable = True\n",
    "\n",
    "#yolo.summary()\n",
    "    \n",
    "# keras.utils.plot_model(\n",
    "#     yolo,\n",
    "#     show_trainable=True\n",
    "#     )\n",
    "    \n",
    "trainable_params = 0 \n",
    "for layer in yolo.trainable_weights:\n",
    "    trainable_params += layer.numpy().size\n",
    "print(F\"Trainable Params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C48F0vhG9vgu"
   },
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(        \n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1 #1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = bestResult #-1\n",
    "        #self.best_map = 999\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        tot = self.data.cardinality().numpy()\n",
    "        i=0\n",
    "        for batch in self.data:\n",
    "            i+=1\n",
    "            print(F\"\\rCalculate Accuracy Batch {i}/{tot}\", end=\"\")\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            y_pred_ragged = bounding_box.to_ragged(y_pred)\n",
    "            self.metrics.update_state(y_true, y_pred_ragged)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = logs[\"MaP\"]\n",
    "        #print(F\"Calculated MaP: {current_map}\")\n",
    "        #current_map = logs[\"val_loss\"]\n",
    "        if current_map > self.best_map and epoch >= EPOCH_MIN_SAVE:\n",
    "            self.best_map = current_map\n",
    "            #Save best MaP ti file\n",
    "            tosave = {\"BestMap\": self.best_map}\n",
    "            with open(F'{self.save_path}.pkl', 'wb') as f:\n",
    "                pickle.dump(tosave, f)\n",
    "            now = datetime.datetime.now()\n",
    "            #self.model.save(self.save_path + \"-test\"+ now.strftime(\"-%m%d%Y-%H%M%S\") + \".h5\")\n",
    "            self.model.save(self.save_path + \".h5\")\n",
    "\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path per i log di TensorBoard\n",
    "#log_dir = \"logs/fit/\" + MODEL_NAME + F\"-Epoch{INITIAL_EPOCH}-{EPOCH}\"# + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = PATH_LOG_TENSORBOARD + \"/fit/\" + MODEL_NAME\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False)\n",
    "\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlPTTQHy9vgu"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# definizione di due callback per migliorare l'addestramento\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"MaP\", patience=30, mode='max', verbose=1)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"MaP\", patience=20, factor=0.80, mode='max')\n",
    "\n",
    "#coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
    "#    val_ds.take(2), bounding_box_format=\"xyxy\"\n",
    "#)\n",
    "\n",
    "history = yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCH,\n",
    "    initial_epoch=INITIAL_EPOCH,\n",
    "    #callbacks=[EvaluateCOCOMetricsCallback(val_ds, MODEL_NAME), tensorboard_callback]\n",
    "    #callbacks=[EvaluateCOCOMetricsCallback(val_ds, MODEL_NAME), tensorboard_callback, early_stopping]\n",
    "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, PATH_MODELS + MODEL_NAME), tensorboard_callback, early_stopping, reduce_lr]\n",
    "    #callbacks=[EvaluateCOCOMetricsCallback(val_ds, MODEL_NAME), tensorboard_callback, reduce_lr]#, early_stopping]\n",
    "    #callbacks=[EvaluateCOCOMetricsCallback(val_ds, MODEL_NAME), reduce_lr]#, early_stopping]\n",
    "    #callbacks=[coco_metrics_callback, tensorboard_callback, reduce_lr]#, early_stopping]\n",
    "    #callbacks=[coco_metrics_callback, reduce_lr]#, early_stopping]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23e4a255f1664c699d2d6cf00253559c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b618213856a34ec3830ad0b48a5a0028",
      "placeholder": "​",
      "style": "IPY_MODEL_8c4cd4fca7d349d5b5d0eb212d5d84f2",
      "value": " 4660/4660 [00:03&lt;00:00, 1138.91it/s]"
     }
    },
    "2b949297991243388c2bbd0a04760f41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a1c6cbfd5c614a68ba9a3bb1f5efc5f7",
       "IPY_MODEL_611f30558a3d4278bb6cdeb52c98cff5",
       "IPY_MODEL_23e4a255f1664c699d2d6cf00253559c"
      ],
      "layout": "IPY_MODEL_6fc913a8f1604a639b5a45437af4718c"
     }
    },
    "5dc5446c9611411ca61b12b715253919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "611f30558a3d4278bb6cdeb52c98cff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dc5446c9611411ca61b12b715253919",
      "max": 4660,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66722e0666bf4bfa96238cdec21491f6",
      "value": 4660
     }
    },
    "66722e0666bf4bfa96238cdec21491f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66b3cacb326448829fc378be51cbeebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc913a8f1604a639b5a45437af4718c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c4cd4fca7d349d5b5d0eb212d5d84f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c6cbfd5c614a68ba9a3bb1f5efc5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3c791bfcf6c453d85dae4f359b74f83",
      "placeholder": "​",
      "style": "IPY_MODEL_66b3cacb326448829fc378be51cbeebe",
      "value": "100%"
     }
    },
    "b618213856a34ec3830ad0b48a5a0028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3c791bfcf6c453d85dae4f359b74f83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
